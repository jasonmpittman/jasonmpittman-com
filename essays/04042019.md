# The Discomfort of AI
## Jason M. Pittman
## April 4th, 2019


I came across this [article](https://arstechnica.com/tech-policy/2019/03/can-ai-be-a-fair-judge-in-court-estonia-thinks-so/) recently discussing the use of narrow AI to automate various government functions, including that of a judge. While the article itself is fairly neutral, the comments surrounding the content are anything else. For example, read through this [Reddit thread](https://www.reddit.com/r/law/comments/b80o6s/can_ai_be_a_fair_judge_in_court_estonia_thinks_so/).

The general sentiment can be summed by a quote from one of the article's subjects: "...Americans have an innate fear of Big Government." I would change that slightly to *humans have an irrational discomfort with AI*. On one hand, I don't find the attitude towards automation or AI at all surprising. Yet, on the other hand, I am surprised given where we clearly are headed technologically. This last notion is what I am going to focus on now.

The source of my surprise is related to my feeling that we (humans) are fundamentally asking the wrong questions about narrow AI.

Here's one example:

**Point**: the sentiment surrounding whether AI can be a fair judge can be boiled down to the idea that AI will not be capable of judging appropriately. More technically, we're troubled with the idea that the technology may not be designed correctly or implemented properly such that fair judgments are meted out. This is a, *should we do this* kind of situation as in the sentiment assumes we haven't done it yet.

**Counterpoint**: well, we have done it. Every time we use Google Maps or any such program we are using essentially the same algorithm. That's right; an AI is determining the optimal path between A and B. Furthermore, it is dynamically guiding us along the way. There's no conversation about such an AI being unfit to make pathing decisions or optimize routes, right? There are discussions around refining the algorithm to increase efficiency and incorporate more environmental factors into the decision-making. These are *how do we do this* type of questions.

Thus, I suspect the insistence of asking *should we do this* rather than *how do we do this* is an irrational discomfort. The true source of the discomfort isn't the narrow AI itself. Our perception of our relationship with the narrow AI is the origin of discomfort. Such discomfort is high enough that we are blind to the fact that narrow AI is already, today infused throughout much of the technology we enjoy on a daily basis.

### Getting it Right

Overall, I think getting a narrow AI right depends on two things. What follows is a summary mind you. There are a variety of details I'm going to not address within each of these dependencies.

First, there is the *correctness* of the AI itself. I would suggest AI should be treated much like cryptography insofar as AI (the algorithm and the data) should be open, transparent, and documented in a clear and concise manner. AI is a gift to humanity and ought to be viewed as such. I should be careful to note here that I don't think narrow AI should be entirely treated like cryptography, however. For instance, the US has tight export controls on cryptography because it is classified as a munition. I suggest treating AI similarly will be disastrous.

Second, if we are sincerely concerned with narrow AI acting as judge, behavioral decision-maker, or medical diagnostician then our arguments should not be about the algorithms at all. Getting narrow AI right, now and in the future, must be about the social benefits and the contextual rationale. Not just dollars savings but how narrow AI improves quality of life enhances our capacity to be *human*.

And who knows what the limits are for that.